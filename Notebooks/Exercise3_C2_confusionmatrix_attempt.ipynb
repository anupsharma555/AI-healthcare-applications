{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the set of labels into a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>perfect_labeler</th>\n",
       "      <th>radiologist</th>\n",
       "      <th>algorithm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cancer</td>\n",
       "      <td>cancer</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cancer</td>\n",
       "      <td>cancer</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cancer</td>\n",
       "      <td>cancer</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cancer</td>\n",
       "      <td>cancer</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cancer</td>\n",
       "      <td>cancer</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  perfect_labeler radiologist  algorithm\n",
       "0          cancer      cancer       0.99\n",
       "1          cancer      cancer       0.94\n",
       "2          cancer      cancer       0.73\n",
       "3          cancer      cancer       0.82\n",
       "4          cancer      cancer       0.98"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start with assessing the radiologist's performance:\n",
    "* Assess the _accuracy_ of the radiologist by just looking at the percent of cases that they correctly labeled\n",
    "* Next, look at the true positive and true negative rates of the radiologist by generating a _confusion matrix_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "radiologist_accuracy = sum(labels.perfect_labeler == labels.radiologist)/len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8993288590604027"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "radiologist_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 25,   4],\n",
       "       [ 11, 109]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(labels.perfect_labeler.values,labels.radiologist.values,labels=[\"cancer\",\"benign\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now look at the algorithm's performance compared to the perfect labeler:\n",
    "* Since the algorithm doesn't create a binary label, it instead returns a _probability_ of cancer, choose a probability cut-off to use for the algorithm's labeling of cancer vs. bening. _(Hint: 0.5 is a reasonable starting place)_\n",
    "* Start with assessing _accuracy_ again here\n",
    "* Generate a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "labels = pd.read_csv('labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Binarize algorithm predictions using a threshold \n",
    "threshold = 0.5 \n",
    "labels['algorithm_pred'] = labels['algorithm'].apply(lambda x: \"cancer\" if x >= threshold else \"benign\")\n",
    "                                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm Accuracy: 0.8859060402684564\n"
     ]
    }
   ],
   "source": [
    "#Step 2: Compute accuracy\n",
    "algorithm_accuracy = accuracy_score(labels['perfect_labeler'], labels['algorithm_pred'])\n",
    "print(\"Algorithm Accuracy:\", algorithm_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[ 21   8]\n",
      " [  9 111]]\n"
     ]
    }
   ],
   "source": [
    "#Step 3: Generate confusion matrix\n",
    "cm = confusion_matrix(labels['perfect_labeler'], labels['algorithm_pred'], labels=[\"cancer\", \"benign\"])\n",
    "print(\"Confusion Matrix: \\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens now if you change the threshold cut-off for your algorithm's classification to 0.4? What if you raise it to 0.6? How do accuracy, fp, fn, tp, and tn change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm Accuracy: 0.9060402684563759\n",
      "Confusion Matrix: \n",
      " [[ 20   9]\n",
      " [  5 115]]\n"
     ]
    }
   ],
   "source": [
    "#Step 1: Binarize algorithm predictions using a threshold \n",
    "threshold = 0.6 \n",
    "labels['algorithm_pred'] = labels['algorithm'].apply(lambda x: \"cancer\" if x >= threshold else \"benign\")\n",
    "\n",
    "#Step 2: Compute accuracy\n",
    "algorithm_accuracy = accuracy_score(labels['perfect_labeler'], labels['algorithm_pred'])\n",
    "print(\"Algorithm Accuracy:\", algorithm_accuracy)\n",
    "\n",
    "#Step 3: Generate confusion matrix\n",
    "cm = confusion_matrix(labels['perfect_labeler'], labels['algorithm_pred'], labels=[\"cancer\", \"benign\"])\n",
    "print(\"Confusion Matrix: \\n\", cm)\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm Accuracy: 0.8590604026845637\n",
      "Confusion Matrix: \n",
      " [[ 25   4]\n",
      " [ 17 103]]\n"
     ]
    }
   ],
   "source": [
    "#Step 1: Binarize algorithm predictions using a threshold \n",
    "threshold = 0.4\n",
    "labels['algorithm_pred'] = labels['algorithm'].apply(lambda x: \"cancer\" if x >= threshold else \"benign\")\n",
    "\n",
    "#Step 2: Compute accuracy\n",
    "algorithm_accuracy = accuracy_score(labels['perfect_labeler'], labels['algorithm_pred'])\n",
    "print(\"Algorithm Accuracy:\", algorithm_accuracy)\n",
    "\n",
    "#Step 3: Generate confusion matrix\n",
    "cm = confusion_matrix(labels['perfect_labeler'], labels['algorithm_pred'], labels=[\"cancer\", \"benign\"])\n",
    "print(\"Confusion Matrix: \\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, let's compare our algorithm to the radiologist\n",
    "* A \"perfect labeler\" might not exist in the real world, and in fact, if often does not\n",
    "* In AI for medical imaging, using a radiologist's labels as our \"true\" label is often the standard of practice, and algorithm performance is judged in both an academic setting as well as in the regulated industry landscape based on performance against an expert human\n",
    "\n",
    "* Repeat the steps above using a set threshold for your algorithm (again, 0.5 is perfectly reasonable) but now computing accuracy, tp, tn, fp, fn against the radiologist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm vs Radiologist Accuracy: 0.8657718120805369\n",
      "Confusion Matrix (vs Radiologist):\n",
      " [[ 23  13]\n",
      " [  7 106]]\n",
      "TP: 23, FN: 13, FP: 7, TN: 106\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Load data\n",
    "labels = pd.read_csv('labels.csv')\n",
    "\n",
    "# Set threshold and compute binary predictions\n",
    "threshold = 0.5\n",
    "labels['algorithm_pred'] = labels['algorithm'].apply(lambda x: \"cancer\" if x >= threshold else \"benign\")\n",
    "\n",
    "# Compare to radiologist instead of perfect_labeler\n",
    "accuracy = accuracy_score(labels['radiologist'], labels['algorithm_pred'])\n",
    "print(\"Algorithm vs Radiologist Accuracy:\", accuracy)\n",
    "\n",
    "# Confusion matrix with radiologist as the reference\n",
    "cm = confusion_matrix(labels['radiologist'], labels['algorithm_pred'], labels=[\"cancer\", \"benign\"])\n",
    "print(\"Confusion Matrix (vs Radiologist):\\n\", cm)\n",
    "\n",
    "# Extract TP, FN, FP, TN\n",
    "TP = cm[0, 0]  # radiologist = cancer, algorithm = cancer\n",
    "FN = cm[0, 1]  # radiologist = cancer, algorithm = benign\n",
    "FP = cm[1, 0]  # radiologist = benign, algorithm = cancer\n",
    "TN = cm[1, 1]  # radiologist = benign, algorithm = benign\n",
    "\n",
    "print(f\"TP: {TP}, FN: {FN}, FP: {FP}, TN: {TN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection: \n",
    "* In the above exercise you assess performances of a human as well as of an algorithm against a 'perfect labeler' and also against each other. \n",
    "* Does accuracy seem like the appropriate statistic to use when evaluating these labels? Why or why not? \n",
    "* In what clinical settings does it seem more or less acceptable to have a high level of FNs? FPs? \n",
    "* How did changing the threshold on the algorithm performance change the different performance statistics? \n",
    "* How did your opinion of the algorithm's performance change when you started comparing it to a radiologist instead of the perfect labeler? What does this mean for a real-world scenario when a perfect labeler doesn't exist, and we only have a radiologist's read to base our performance on? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy is insufficient alone and metrics need to be aligned with clinical risks of the testing. \n",
    "FP vs. FP tolerance depends on clinical context and downstream consequences. \n",
    "Threshold tuing offers control over trade-offs in sensitivity / specificity. \n",
    "When the \"truth\" is expert opinion, benchmarking against radiologists or other experts is both realistic and meaningful.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
