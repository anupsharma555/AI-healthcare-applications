{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Final Project Instructions (Detecting Pneumonia Detection from Chest X-rays)\n",
    "\n",
    "Pneumonia Detection from Chest X-Rays\n",
    "Project Overview\n",
    "In this project, you will apply the skills that you have acquired in this 2D medical imaging course to analyze data from the NIH Chest X-ray Dataset and train a CNN to classify a given chest x-ray for the presence or absence of pneumonia. This project will culminate in a model that can predict the presence of pneumonia with human radiologist-level accuracy that can be prepared for submission to the FDA for 510(k) clearance as software as a medical device. As part of the submission preparation, you will formally describe your model, the data that it was trained on, and a validation plan that meets FDA needs.\n",
    "\n",
    "You will be provided with the medical images with clinical labels for each image that were extracted from their accompanying radiology reports.\n",
    "\n",
    "The project will include access to a GPU for fast training of deep learning architecture, as well as access to 112,000 chest x-rays with disease labels acquired from 30,000 patients.\n",
    "\n",
    "Pneumonia and X-Rays in the World\n",
    "Chest X-ray exams are one of the most frequent and cost-effective types of medical imaging examinations. Deriving clinical diagnoses from chest X-rays can be challenging, however, even by skilled radiologists.\n",
    "\n",
    "When it comes to pneumonia, chest X-rays are the best available method for diagnosis. More than 1 million adults are hospitalized with pneumonia and around 50,000 die from the disease every year in the US alone. The high prevalence of pneumonia makes it a good candidate for the development of a deep learning application for two reasons: 1) Data availability in a high enough quantity for training deep learning models for image classification 2) Opportunity for clinical aid by providing higher accuracy image reads of a difficult-to-diagnose disease and/or reduce clinical burnout by performing automated reads of very common scans.\n",
    "\n",
    "The diagnosis of pneumonia from chest X-rays is difficult for several reasons:\n",
    "\n",
    "The appearance of pneumonia in a chest X-ray can be very vague depending on the stage of the infection\n",
    "Pneumonia often overlaps with other diagnoses\n",
    "Pneumonia can mimic benign abnormalities\n",
    "For these reasons, common methods of diagnostic validation performed in the clinical setting are to obtain sputum cultures to test for the presence of bacteria or viral bodies that cause pneumonia, reading the patient's clinical history and taking their demographic profile into account, and comparing a current image to prior chest X-rays for the same patient if they are available.\n",
    "\n",
    "About the Dataset\n",
    "The dataset provided to you for this project was curated by the NIH specifically to address the problem of a lack of large x-ray datasets with ground truth labels to be used in the creation of disease detection algorithms.\n",
    "\n",
    "The data is mounted in the Udacity Jupyter GPU workspace provided to you, along with code to load the data. Alternatively, you can download the data from the kaggle website or official NIH website and run it locally. You are STRONGLY recommended to complete the project using the Udacity workspace since the data is huge, and you will need GPU to accelerate the training process.\n",
    "\n",
    "There are 112,120 X-ray images with disease labels from 30,805 unique patients in this dataset. The disease labels were created using Natural Language Processing (NLP) to mine the associated radiological reports. The labels include 14 common thoracic pathologies:\n",
    "\n",
    "Atelectasis\n",
    "Consolidation\n",
    "Infiltration\n",
    "Pneumothorax\n",
    "Edema\n",
    "Emphysema\n",
    "Fibrosis\n",
    "Effusion\n",
    "Pneumonia\n",
    "Pleural thickening\n",
    "Cardiomegaly\n",
    "Nodule\n",
    "Mass\n",
    "Hernia\n",
    "The biggest limitation of this dataset is that image labels were NLP-extracted so there could be some erroneous labels but the NLP labeling accuracy is estimated to be >90%.\n",
    "\n",
    "The original radiology reports are not publicly available but you can find more details on the labeling process here.\n",
    "\n",
    "Dataset Contents:\n",
    "112,120 frontal-view chest X-ray PNG images in 1024*1024 resolution (under images folder)\n",
    "Meta data for all images (Data_Entry_2017.csv): Image Index, Finding Labels, Follow-up #, Patient ID, Patient Age, Patient Gender, View Position, Original Image Size and Original Image Pixel Spacing.\n",
    "\n",
    "Project Steps\n",
    "1. Exploratory Data Analysis\n",
    "The first part of this project will involve exploratory data analysis (EDA) to understand and describe the content and nature of the data.\n",
    "\n",
    "Note that much of the work performed during your EDA will enable the completion of the final component of this project which is focused on documentation of your algorithm for the FDA. This is described in a later section, but some important things to focus on during your EDA may be:\n",
    "\n",
    "The patient demographic data such as gender, age, patient position,etc. (as it is available)\n",
    "The x-ray views taken (i.e. view position)\n",
    "The number of cases including:\n",
    "number of pneumonia cases,\n",
    "number of non-pneumonia cases\n",
    "The distribution of other diseases that are comorbid with pneumonia\n",
    "Number of disease per patient\n",
    "Pixel-level assessments of the imaging data for healthy & disease states of interest (e.g. histograms of intensity values) and compare distributions across diseases.\n",
    "2. Building and Training Your Model\n",
    "Training and validating Datasets\n",
    "\n",
    "From your findings in the EDA component of this project, curate the appropriate training and validation sets for classifying pneumonia. Be sure to take the following into consideration:\n",
    "\n",
    "Distribution of diseases other than pneumonia that are present in both datasets\n",
    "Demographic information, image view positions, and number of images per patient in each set\n",
    "Distribution of pneumonia-positive and pneumonia-negative cases in each dataset\n",
    "Model Architecture\n",
    "\n",
    "In this project, you will fine-tune an existing CNN architecture to classify x-rays images for the presence of pneumonia. There is no required architecture required for this project, but a reasonable choice would be using the VGG16 architecture with weights trained on the ImageNet dataset. Fine-tuning can be performed by freezing your chosen pre-built network and adding several new layers to the end to train, or by doing this in combination with selectively freezing and training some layers of the pre-trained network.\n",
    "\n",
    "Image Pre-Processing and Augmentation\n",
    "\n",
    "You may choose or need to do some amount of preprocessing prior to feeding imagees into your network for training and validating. This may serve the purpose of conforming to your model's architecture and/or for the purposes of augmenting your training dataset for increasing your model performance. When performing image augmentation, be sure to think about augmentation parameters that reflect real-world differences that may be seen in chest X-rays.\n",
    "\n",
    "Training\n",
    "\n",
    "In training your model, there are many parameters that can be tweaked to improve performance including:\n",
    "\n",
    "Image augmentation parameters\n",
    "Training batch size\n",
    "Training learning rate\n",
    "Inclusion and parameters of specific layers in your model\n",
    "You will be asked to provide descriptions of the methods by which given parameters were chosen in the final FDA documentation.\n",
    "\n",
    "Performance Assessment\n",
    "\n",
    "As you train your model, you will monitor its performance over subsequence training epochs. Choose the appropriate metrics upon which to monitor performance. Note that 'accuracy' may not be the most appropriate statistic in this case, depending on the balance or imbalance of your validation dataset, and also depending on the clinical context that you want to use this model in (i.e. can you sacrafice high false positive rate for a low false negative rate?)\n",
    "\n",
    "Note that detecting pneumonia is hard even for trained expert radiologists, so you should not expect to acheive sky-high performance. This paper describes some human-reader-level F1 scores for detecting pneumonia, and can be used as a reference point for how well your model could perform.\n",
    "\n",
    "3. Clinical Workflow Integration\n",
    "The imaging data provided to you for training your model was transformed from DICOM format into .png to help aid in the image pre-processing and model training steps of this project. In the real world, however, the pixel-level imaging data are contained inside of standard DICOM files.\n",
    "\n",
    "For this project, create a DICOM wrapper that takes in a standard DICOM file and outputs data in the format accepted by your model. Be sure to include several checks in your wrapper for the following:\n",
    "\n",
    "Proper image acquisition type (i.e. X-ray)\n",
    "Proper image acquisition orientation (i.e. those present in your training data)\n",
    "Proper body part in acquisition\n",
    "\n",
    "4. FDA Submission\n",
    "For this project, you will complete the following steps that are derived from the FDA's official guidance on both the algorithm description and the algorithm performance assessment. Much of this portion of the project relies on what you did during your EDA, model building, and model training. Use figures and statistics from those earlier parts in completing the following documentation.\n",
    "\n",
    "1. General Information:\n",
    "First, provide an Intended Use statement for your model\n",
    "Then, provide some indications for use that should include:\n",
    "Target population\n",
    "When your device could be utilized within a clinical workflow\n",
    "Device limitations, including diseases/conditions/abnormalities for which the device has been found ineffective and should not be used\n",
    "Explain how a false positive or false negative might impact a patient\n",
    "2. Algorithm Design and Function\n",
    "\n",
    "In this section, describe your fully trained algorithm and the DICOM header checks that you have built around it. Include a flowchart that describes the following:\n",
    "\n",
    "Any pre-algorithm checks you perform on your DICOM\n",
    "Any preprocessing steps performed by your algorithm on the original images (e.g. normalization)\n",
    "Note that this section should not include augmentation\n",
    "The architecture of the classifier\n",
    "For each stage of your algorithm, briefly describe the design and function.\n",
    "\n",
    "3. Algorithm Training\n",
    "\n",
    "Describe the following parameters of your algorithm and how they were chosen:\n",
    "\n",
    "Types of augmentation used during training\n",
    "Batch size\n",
    "Optimizer learning rate\n",
    "Layers of pre-existing architecture that were frozen\n",
    "Layers of pre-existing architecture that were fine-tuned\n",
    "Layers added to pre-existing architecture\n",
    "Also describe the behavior of the following throughout training (use visuals to show):\n",
    "\n",
    "Training loss\n",
    "Validation loss\n",
    "Describe the algorithm's final performance after training was complete by showing a precision-recall curve on your validation set.\n",
    "\n",
    "Finally, report the threshold for classification that you chose and the corresponded F1 score, recall, and precision. Give one or two sentences of explanation for why you chose this threshold value.\n",
    "\n",
    "4. Databases\n",
    "\n",
    "For the database of patient data used, provide specific information about the training and validation datasets that you curated separately, including:\n",
    "\n",
    "Size of the dataset\n",
    "The number of positive cases and the its radio to the number of negative cases\n",
    "The patient demographic data (as it is available)\n",
    "The radiologic techniques used and views taken\n",
    "The co-occurrence frequencies of pneumonia with other diseases and findings\n",
    "\n",
    "5. Ground Truth\n",
    "\n",
    "The methodology used to establish the ground truth can impact reported performance. Describe how the NIH created the ground truth for the data that was provided to you for this project. Describe the benefits and limitations of this type of ground truth.\n",
    "\n",
    "6. FDA Validation Plan\n",
    "\n",
    "You will simply describe how a FDA Validation Plan would be conducted for your algorithm, rather than actually performing the assessment. Describe the following:\n",
    "\n",
    "The patient population that you would request imaging data from from your clinical partner. Make sure to include:\n",
    "\n",
    "Age ranges\n",
    "Sex\n",
    "Type of imaging modality\n",
    "Body part imaged\n",
    "Prevalence of disease of interest\n",
    "Any other diseases that should be included or excluded as comorbidities in the population\n",
    "Provide a short explanation of how you would obtain an optimal ground truth\n",
    "\n",
    "Provide a performance standard that you choose based on this paper: Link: https://arxiv.org/pdf/1711.05225\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skeleton Code\n",
    "\n",
    "The code below provides a skeleton for the model building & training component of your project. You can add/remove/build on code however you see fit, this is meant as a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Import Core libraries \n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "### Import any other stats/DL/ML packages you may need here. E.g. Keras, scikit-learn, etc.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, classification_report, confusion_matrix, f1_score, accuracy_score, precision_score, recall_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16 \n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, GlobalAveragePooling2D, Input\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import itertools\n",
    "import random\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do some early processing of your metadata for easier model training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Below is some helper code to read all of your full image filepaths into a dataframe for easier manipulation\n",
    "## Load the NIH data to all_xray_df\n",
    "all_xray_df = pd.read_csv('/data/Data_Entry_2017.csv')\n",
    "all_image_paths = {os.path.basename(x): x for x in \n",
    "                   glob(os.path.join('/data','images*', '*', '*.png'))}\n",
    "print('Scans found:', len(all_image_paths), ', Total Headers', all_xray_df.shape[0])\n",
    "all_xray_df['path'] = all_xray_df['Image Index'].map(all_image_paths.get)\n",
    "all_xray_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here you may want to create some extra columns in your table with binary indicators of certain diseases \n",
    "## rather than working directly with the 'Finding Labels' column\n",
    "\n",
    "# Todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we can create a new column called 'pneumonia_class' that will allow us to look at \n",
    "## images with or without pneumonia for binary classification\n",
    "\n",
    "# Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your training and testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_splits(vargs):\n",
    "    \n",
    "    ## Either build your own or use a built-in library to split your original dataframe into two sets \n",
    "    ## that can be used for training and testing your model\n",
    "    ## It's important to consider here how balanced or imbalanced you want each of those sets to be\n",
    "    ## for the presence of pneumonia\n",
    "    \n",
    "    # Todo\n",
    "    \n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we can begin our model-building & training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First suggestion: perform some image augmentation on your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_image_augmentation(vargs):\n",
    "    \n",
    "    ## recommendation here to implement a package like Keras' ImageDataGenerator\n",
    "    ## with some of the built-in augmentations \n",
    "    \n",
    "    ## keep an eye out for types of augmentation that are or are not appropriate for medical imaging data\n",
    "    ## Also keep in mind what sort of augmentation is or is not appropriate for testing vs validation data\n",
    "    \n",
    "    ## STAND-OUT SUGGESTION: implement some of your own custom augmentation that's *not*\n",
    "    ## built into something like a Keras package\n",
    "    \n",
    "    # Todo\n",
    "    \n",
    "    return my_idg\n",
    "\n",
    "\n",
    "def make_train_gen(vargs):\n",
    "    \n",
    "    ## Create the actual generators using the output of my_image_augmentation for your training data\n",
    "    ## Suggestion here to use the flow_from_dataframe library, e.g.:\n",
    "    \n",
    "#     train_gen = my_train_idg.flow_from_dataframe(dataframe=train_df, \n",
    "#                                          directory=None, \n",
    "#                                          x_col = ,\n",
    "#                                          y_col = ,\n",
    "#                                          class_mode = 'binary',\n",
    "#                                          target_size = , \n",
    "#                                          batch_size = \n",
    "#                                          )\n",
    "     # Todo\n",
    "\n",
    "    return train_gen\n",
    "\n",
    "\n",
    "def make_val_gen(vargs):\n",
    "    \n",
    "#     val_gen = my_val_idg.flow_from_dataframe(dataframe = val_data, \n",
    "#                                              directory=None, \n",
    "#                                              x_col = ,\n",
    "#                                              y_col = ',\n",
    "#                                              class_mode = 'binary',\n",
    "#                                              target_size = , \n",
    "#                                              batch_size = ) \n",
    "    \n",
    "    # Todo\n",
    "    return val_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## May want to pull a single large batch of random validation data for testing after each epoch:\n",
    "valX, valY = val_gen.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## May want to look at some examples of our augmented training data. \n",
    "## This is helpful for understanding the extent to which data is being manipulated prior to training, \n",
    "## and can be compared with how the raw data look prior to augmentation\n",
    "\n",
    "t_x, t_y = next(train_gen)\n",
    "fig, m_axs = plt.subplots(4, 4, figsize = (16, 16))\n",
    "for (c_x, c_y, c_ax) in zip(t_x, t_y, m_axs.flatten()):\n",
    "    c_ax.imshow(c_x[:,:,0], cmap = 'bone')\n",
    "    if c_y == 1: \n",
    "        c_ax.set_title('Pneumonia')\n",
    "    else:\n",
    "        c_ax.set_title('No Pneumonia')\n",
    "    c_ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build your model: \n",
    "\n",
    "Recommendation here to use a pre-trained network downloaded from Keras for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_model(vargs):\n",
    "    \n",
    "    # model = VGG16(include_top=True, weights='imagenet')\n",
    "    # transfer_layer = model.get_layer(lay_of_interest)\n",
    "    # vgg_model = Model(inputs = model.input, outputs = transfer_layer.output)\n",
    "    \n",
    "    # Todo\n",
    "    \n",
    "    return vgg_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_my_model(vargs):\n",
    "    \n",
    "    # my_model = Sequential()\n",
    "    # ....add your pre-trained model, and then whatever additional layers you think you might\n",
    "    # want for fine-tuning (Flatteen, Dense, Dropout, etc.)\n",
    "    \n",
    "    # if you want to compile your model within this function, consider which layers of your pre-trained model, \n",
    "    # you want to freeze before you compile \n",
    "    \n",
    "    # also make sure you set your optimizer, loss function, and metrics to monitor\n",
    "    \n",
    "    # Todo\n",
    "    \n",
    "    return my_model\n",
    "\n",
    "\n",
    "\n",
    "## STAND-OUT Suggestion: choose another output layer besides just the last classification layer of your modele\n",
    "## to output class activation maps to aid in clinical interpretation of your model's results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Below is some helper code that will allow you to add checkpoints to your model,\n",
    "## This will save the 'best' version of your model by comparing it to previous epochs of training\n",
    "\n",
    "## Note that you need to choose which metric to monitor for your model's 'best' performance if using this code. \n",
    "## The 'patience' parameter is set to 10, meaning that your model will train for ten epochs without seeing\n",
    "## improvement before quitting\n",
    "\n",
    "# Todo\n",
    "\n",
    "# weight_path=\"{}_my_model.best.hdf5\".format('xray_class')\n",
    "\n",
    "# checkpoint = ModelCheckpoint(weight_path, \n",
    "#                              monitor= CHOOSE_METRIC_TO_MONITOR_FOR_PERFORMANCE, \n",
    "#                              verbose=1, \n",
    "#                              save_best_only=True, \n",
    "#                              mode= CHOOSE_MIN_OR_MAX_FOR_YOUR_METRIC, \n",
    "#                              save_weights_only = True)\n",
    "\n",
    "# early = EarlyStopping(monitor= SAME_AS_METRIC_CHOSEN_ABOVE, \n",
    "#                       mode= CHOOSE_MIN_OR_MAX_FOR_YOUR_METRIC, \n",
    "#                       patience=10)\n",
    "\n",
    "# callbacks_list = [checkpoint, early]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train your model\n",
    "\n",
    "# Todo\n",
    "\n",
    "# history = my_model.fit_generator(train_gen, \n",
    "#                           validation_data = (valX, valY), \n",
    "#                           epochs = , \n",
    "#                           callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### After training for some time, look at the performance of your model by plotting some performance statistics:\n",
    "\n",
    "Note, these figures will come in handy for your FDA documentation later in the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## After training, make some predictions to assess your model's overall performance\n",
    "## Note that detecting pneumonia is hard even for trained expert radiologists, \n",
    "## so there is no need to make the model perfect.\n",
    "my_model.load_weights(weight_path)\n",
    "pred_Y = new_model.predict(valX, batch_size = 32, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_auc(t_y, p_y):\n",
    "    \n",
    "    ## Hint: can use scikit-learn's built in functions here like roc_curve\n",
    "    \n",
    "    # Todo\n",
    "    \n",
    "    return\n",
    "\n",
    "## what other performance statistics do you want to include here besides AUC? \n",
    "\n",
    "\n",
    "# def ... \n",
    "# Todo\n",
    "\n",
    "# def ...\n",
    "# Todo\n",
    "    \n",
    "#Also consider plotting the history of your model training:\n",
    "\n",
    "def plot_history(history):\n",
    "    \n",
    "    # Todo\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot figures\n",
    "\n",
    "# Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you feel you are done training, you'll need to decide the proper classification threshold that optimizes your model's performance for a given metric (e.g. accuracy, F1, precision, etc.  You decide) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the threshold that optimize your model's performance,\n",
    "## and use that threshold to make binary classification. Make sure you take all your metrics into consideration.\n",
    "\n",
    "# Todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's look at some examples of true vs. predicted with our best model: \n",
    "\n",
    "# Todo\n",
    "\n",
    "# fig, m_axs = plt.subplots(10, 10, figsize = (16, 16))\n",
    "# i = 0\n",
    "# for (c_x, c_y, c_ax) in zip(valX[0:100], testY[0:100], m_axs.flatten()):\n",
    "#     c_ax.imshow(c_x[:,:,0], cmap = 'bone')\n",
    "#     if c_y == 1: \n",
    "#         if pred_Y[i] > YOUR_THRESHOLD:\n",
    "#             c_ax.set_title('1, 1')\n",
    "#         else:\n",
    "#             c_ax.set_title('1, 0')\n",
    "#     else:\n",
    "#         if pred_Y[i] > YOUR_THRESHOLD: \n",
    "#             c_ax.set_title('0, 1')\n",
    "#         else:\n",
    "#             c_ax.set_title('0, 0')\n",
    "#     c_ax.axis('off')\n",
    "#     i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Just save model architecture to a .json:\n",
    "\n",
    "model_json = my_model.to_json()\n",
    "with open(\"my_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anup_prompt_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
